#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä API –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤.

–£–ª—É—á—à–µ–Ω–∏—è:
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–Ω—ã—Ö –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–∏–∑–º–µ–Ω—è–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–µ–π –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ rate limits

–ê–≤—Ç–æ—Ä: ETL System
–î–∞—Ç–∞: 06 —è–Ω–≤–∞—Ä—è 2025
"""

import asyncio
import aiohttp
import time
import json
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict, deque
import pickle
import os

logger = logging.getLogger(__name__)


class CacheType(Enum):
    """–¢–∏–ø—ã –∫—ç—à–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    PRODUCT_INFO = "product_info"
    WAREHOUSE_LIST = "warehouse_list"
    CATEGORY_INFO = "category_info"
    RATE_LIMITS = "rate_limits"


@dataclass
class CacheEntry:
    """–ó–∞–ø–∏—Å—å –≤ –∫—ç—à–µ."""
    data: Any
    created_at: datetime
    expires_at: datetime
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    
    def is_expired(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–µ—á–µ–Ω–∏—è —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è."""
        return datetime.now() > self.expires_at
    
    def access(self) -> Any:
        """–î–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        self.access_count += 1
        self.last_accessed = datetime.now()
        return self.data


@dataclass
class RateLimitInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª–∏–º–∏—Ç–∞—Ö API."""
    requests_per_second: float
    requests_per_minute: int
    requests_per_hour: int
    current_requests: int = 0
    reset_time: Optional[datetime] = None
    last_request_time: Optional[datetime] = None


@dataclass
class BatchConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–∞—Ç—á–µ–π –¥–ª—è API."""
    initial_size: int
    min_size: int
    max_size: int
    adaptive: bool = True
    success_threshold: float = 0.95
    error_threshold: float = 0.1


class APIRequestOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä API –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""
    
    def __init__(self, cache_dir: str = "cache", max_cache_size: int = 1000):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞.
        
        Args:
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞
            max_cache_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ –≤ –∑–∞–ø–∏—Å—è—Ö
        """
        self.cache_dir = cache_dir
        self.max_cache_size = max_cache_size
        self._cache: Dict[str, CacheEntry] = {}
        self._cache_lock = threading.Lock()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'api_requests': 0,
            'parallel_requests': 0,
            'batch_optimizations': 0
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –±–∞—Ç—á–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö API
        self._batch_configs = {
            'ozon_stocks': BatchConfig(1000, 100, 2000),
            'ozon_products': BatchConfig(500, 50, 1000),
            'wb_stocks': BatchConfig(1000, 100, 2000),
            'wb_products': BatchConfig(500, 50, 1000)
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ rate limits
        self._rate_limits = {
            'ozon': RateLimitInfo(10.0, 600, 7200),  # 10 RPS, 600 RPM, 7200 RPH
            'wb': RateLimitInfo(5.0, 300, 3600)      # 5 RPS, 300 RPM, 3600 RPH
        }
        
        # –û—á–µ—Ä–µ–¥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è rate limiting
        self._request_queues = {
            'ozon': deque(),
            'wb': deque()
        }
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∫—ç—à–∞
        os.makedirs(cache_dir, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –∏–∑ —Ñ–∞–π–ª–∞
        self._load_cache_from_disk()
    
    def _generate_cache_key(self, cache_type: CacheType, **kwargs) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞."""
        key_data = f"{cache_type.value}:{json.dumps(kwargs, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _load_cache_from_disk(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ —Å –¥–∏—Å–∫–∞."""
        cache_file = os.path.join(self.cache_dir, "api_cache.pkl")
        
        try:
            if os.path.exists(cache_file):
                with open(cache_file, 'rb') as f:
                    disk_cache = pickle.load(f)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∏—Å—Ç–µ–∫—à–∏–µ –∑–∞–ø–∏—Å–∏
                current_time = datetime.now()
                for key, entry in disk_cache.items():
                    if not entry.is_expired():
                        self._cache[key] = entry
                
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self._cache)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
    
    def _save_cache_to_disk(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –Ω–∞ –¥–∏—Å–∫."""
        cache_file = os.path.join(self.cache_dir, "api_cache.pkl")
        
        try:
            with self._cache_lock:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∏—Å—Ç–µ–∫—à–∏–µ –∑–∞–ø–∏—Å–∏
                valid_cache = {
                    key: entry for key, entry in self._cache.items()
                    if not entry.is_expired()
                }
                
                with open(cache_file, 'wb') as f:
                    pickle.dump(valid_cache, f)
                
                logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(valid_cache)} –∑–∞–ø–∏—Å–µ–π –≤ –∫—ç—à")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def get_cached_data(self, cache_type: CacheType, **kwargs) -> Optional[Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞.
        
        Args:
            cache_type: –¢–∏–ø –∫—ç—à–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞
            
        Returns:
            –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None
        """
        cache_key = self._generate_cache_key(cache_type, **kwargs)
        
        with self._cache_lock:
            if cache_key in self._cache:
                entry = self._cache[cache_key]
                
                if not entry.is_expired():
                    self._stats['cache_hits'] += 1
                    return entry.access()
                else:
                    # –£–¥–∞–ª—è–µ–º –∏—Å—Ç–µ–∫—à—É—é –∑–∞–ø–∏—Å—å
                    del self._cache[cache_key]
            
            self._stats['cache_misses'] += 1
            return None
    
    def set_cached_data(self, cache_type: CacheType, data: Any, ttl_hours: int = 24, **kwargs) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à.
        
        Args:
            cache_type: –¢–∏–ø –∫—ç—à–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
            ttl_hours: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –≤ —á–∞—Å–∞—Ö
            **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞
        """
        cache_key = self._generate_cache_key(cache_type, **kwargs)
        
        with self._cache_lock:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ –∏ –æ—á–∏—â–∞–µ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if len(self._cache) >= self.max_cache_size:
                self._cleanup_cache()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            entry = CacheEntry(
                data=data,
                created_at=datetime.now(),
                expires_at=datetime.now() + timedelta(hours=ttl_hours)
            )
            
            self._cache[cache_key] = entry
    
    def _cleanup_cache(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –æ—Ç —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π."""
        current_time = datetime.now()
        
        # –£–¥–∞–ª—è–µ–º –∏—Å—Ç–µ–∫—à–∏–µ –∑–∞–ø–∏—Å–∏
        expired_keys = [
            key for key, entry in self._cache.items()
            if entry.is_expired()
        ]
        
        for key in expired_keys:
            del self._cache[key]
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç, —É–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ
        if len(self._cache) >= self.max_cache_size:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            sorted_entries = sorted(
                self._cache.items(),
                key=lambda x: (x[1].access_count, x[1].last_accessed or x[1].created_at)
            )
            
            # –£–¥–∞–ª—è–µ–º 20% –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π
            to_remove = int(len(sorted_entries) * 0.2)
            for key, _ in sorted_entries[:to_remove]:
                del self._cache[key]
        
        logger.debug(f"üßπ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞: –æ—Å—Ç–∞–ª–æ—Å—å {len(self._cache)} –∑–∞–ø–∏—Å–µ–π")
    
    def _check_rate_limit(self, marketplace: str) -> float:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ rate limit –∏ –≤–æ–∑–≤—Ä–∞—Ç –≤—Ä–µ–º–µ–Ω–∏ –æ–∂–∏–¥–∞–Ω–∏—è.
        
        Args:
            marketplace: –ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å ('ozon' –∏–ª–∏ 'wb')
            
        Returns:
            –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        if marketplace not in self._rate_limits:
            return 0.0
        
        rate_info = self._rate_limits[marketplace]
        current_time = datetime.now()
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        queue = self._request_queues[marketplace]
        while queue and (current_time - queue[0]).total_seconds() > 60:
            queue.popleft()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É
        if len(queue) >= rate_info.requests_per_minute:
            # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–æ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è —Å–ª–æ—Ç–∞
            oldest_request = queue[0]
            wait_time = 60 - (current_time - oldest_request).total_seconds()
            return max(0, wait_time)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
        recent_requests = [
            req_time for req_time in queue
            if (current_time - req_time).total_seconds() <= 1
        ]
        
        if len(recent_requests) >= rate_info.requests_per_second:
            return 1.0 / rate_info.requests_per_second
        
        return 0.0
    
    def _record_request(self, marketplace: str) -> None:
        """–ó–∞–ø–∏—Å—å –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞."""
        if marketplace in self._request_queues:
            self._request_queues[marketplace].append(datetime.now())
            self._stats['api_requests'] += 1
    
    async def make_api_request(
        self,
        session: aiohttp.ClientSession,
        method: str,
        url: str,
        marketplace: str,
        cache_type: Optional[CacheType] = None,
        cache_ttl: int = 24,
        **kwargs
    ) -> Optional[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ API –∑–∞–ø—Ä–æ—Å–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ rate limiting.
        
        Args:
            session: HTTP —Å–µ—Å—Å–∏—è
            method: HTTP –º–µ—Ç–æ–¥
            url: URL –∑–∞–ø—Ä–æ—Å–∞
            marketplace: –ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å –¥–ª—è rate limiting
            cache_type: –¢–∏–ø –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
            cache_ttl: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —á–∞—Å–∞—Ö
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            –û—Ç–≤–µ—Ç API –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è GET –∑–∞–ø—Ä–æ—Å–æ–≤
        if method.upper() == 'GET' and cache_type:
            cached_data = self.get_cached_data(cache_type, url=url, **kwargs.get('params', {}))
            if cached_data:
                return cached_data
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º rate limit
        wait_time = self._check_rate_limit(marketplace)
        if wait_time > 0:
            logger.debug(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ rate limit –¥–ª—è {marketplace}: {wait_time:.2f} —Å–µ–∫")
            await asyncio.sleep(wait_time)
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            async with session.request(method, url, **kwargs) as response:
                self._record_request(marketplace)
                
                if response.status == 200:
                    data = await response.json()
                    
                    # –ö—ç—à–∏—Ä—É–µ–º GET –∑–∞–ø—Ä–æ—Å—ã
                    if method.upper() == 'GET' and cache_type:
                        self.set_cached_data(cache_type, data, cache_ttl, url=url, **kwargs.get('params', {}))
                    
                    return data
                
                elif response.status == 429:  # Too Many Requests
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ rate limits
                    retry_after = int(response.headers.get('Retry-After', 60))
                    logger.warning(f"‚ö†Ô∏è Rate limit –¥–ª—è {marketplace}, –æ–∂–∏–¥–∞–Ω–∏–µ {retry_after} —Å–µ–∫")
                    await asyncio.sleep(retry_after)
                    return None
                
                else:
                    logger.error(f"‚ùå API –æ—à–∏–±–∫–∞ {response.status}: {await response.text()}")
                    return None
        
        except asyncio.TimeoutError:
            logger.error(f"‚ùå Timeout –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url}")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ API –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return None
    
    async def fetch_data_parallel(
        self,
        requests: List[Dict[str, Any]],
        max_concurrent: int = 10
    ) -> List[Optional[Dict[str, Any]]]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö API –∑–∞–ø—Ä–æ—Å–æ–≤.
        
        Args:
            requests: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def bounded_request(request_params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            async with semaphore:
                async with aiohttp.ClientSession() as session:
                    return await self.make_api_request(session, **request_params)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [bounded_request(req) for req in requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)
        
        self._stats['parallel_requests'] += len(requests)
        return processed_results
    
    def optimize_batch_size(self, api_type: str, success_rate: float, avg_response_time: float) -> int:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
        
        Args:
            api_type: –¢–∏–ø API
            success_rate: –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            avg_response_time: –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
            
        Returns:
            –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        """
        if api_type not in self._batch_configs:
            return 1000  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        config = self._batch_configs[api_type]
        
        if not config.adaptive:
            return config.initial_size
        
        current_size = config.initial_size
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–∏ —Ö–æ—Ä–æ—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if success_rate >= config.success_threshold and avg_response_time < 2.0:
            new_size = min(int(current_size * 1.2), config.max_size)
        
        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–∏ –ø–ª–æ—Ö–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        elif success_rate < config.error_threshold or avg_response_time > 10.0:
            new_size = max(int(current_size * 0.8), config.min_size)
        
        # –°–ª–µ–≥–∫–∞ —É–º–µ–Ω—å—à–∞–µ–º –ø—Ä–∏ —Å—Ä–µ–¥–Ω–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        elif success_rate < config.success_threshold:
            new_size = max(int(current_size * 0.9), config.min_size)
        
        else:
            new_size = current_size
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config.initial_size = new_size
        self._stats['batch_optimizations'] += 1
        
        logger.debug(f"üìä –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞ {api_type}: {current_size} -> {new_size} "
                    f"(success: {success_rate:.2%}, time: {avg_response_time:.2f}s)")
        
        return new_size
    
    async def fetch_ozon_stocks_optimized(
        self,
        client_id: str,
        api_key: str,
        base_url: str
    ) -> List[Dict[str, Any]]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ —Å Ozon API.
        
        Args:
            client_id: Client ID –¥–ª—è Ozon API
            api_key: API –∫–ª—é—á –¥–ª—è Ozon API
            base_url: –ë–∞–∑–æ–≤—ã–π URL API
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ—Å—Ç–∞—Ç–∫–∞–º–∏
        """
        all_items = []
        batch_size = self._batch_configs['ozon_stocks'].initial_size
        
        headers = {
            "Client-Id": client_id,
            "Api-Key": api_key,
            "Content-Type": "application/json"
        }
        
        async with aiohttp.ClientSession() as session:
            url = f"{base_url}/v3/product/info/stocks"
            offset = 0
            
            while True:
                start_time = time.time()
                
                payload = {
                    "filter": {},
                    "last_id": "",
                    "limit": batch_size
                }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                cache_key = f"ozon_stocks_{offset}_{batch_size}"
                cached_data = self.get_cached_data(
                    CacheType.PRODUCT_INFO,
                    endpoint="stocks",
                    offset=offset,
                    limit=batch_size
                )
                
                if cached_data:
                    items = cached_data.get('items', [])
                    logger.debug(f"üì¶ –ü–æ–ª—É—á–µ–Ω–æ {len(items)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∫—ç—à–∞ (offset: {offset})")
                else:
                    # –í—ã–ø–æ–ª–Ω—è–µ–º API –∑–∞–ø—Ä–æ—Å
                    response_data = await self.make_api_request(
                        session=session,
                        method='POST',
                        url=url,
                        marketplace='ozon',
                        json=payload,
                        headers=headers,
                        timeout=30
                    )
                    
                    if not response_data:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å offset {offset}")
                        break
                    
                    items = response_data.get('result', {}).get('items', [])
                    
                    # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ 1 —á–∞—Å
                    self.set_cached_data(
                        CacheType.PRODUCT_INFO,
                        {'items': items},
                        ttl_hours=1,
                        endpoint="stocks",
                        offset=offset,
                        limit=batch_size
                    )
                    
                    logger.info(f"üì¶ –ü–æ–ª—É—á–µ–Ω–æ {len(items)} —Ç–æ–≤–∞—Ä–æ–≤ —Å Ozon API (offset: {offset})")
                
                if not items:
                    break
                
                all_items.extend(items)
                
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                response_time = time.time() - start_time
                success_rate = 1.0 if items else 0.0
                
                new_batch_size = self.optimize_batch_size('ozon_stocks', success_rate, response_time)
                if new_batch_size != batch_size:
                    batch_size = new_batch_size
                    logger.info(f"üîß –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ {batch_size}")
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ –ª–∏–º–∏—Ç–∞, —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                if len(items) < batch_size:
                    break
                
                offset += len(items)
        
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_items)} —Ç–æ–≤–∞—Ä–æ–≤ —Å Ozon")
        return all_items
    
    async def fetch_wb_stocks_optimized(
        self,
        api_token: str,
        base_url: str
    ) -> List[Dict[str, Any]]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ —Å Wildberries API.
        
        Args:
            api_token: API —Ç–æ–∫–µ–Ω –¥–ª—è WB
            base_url: –ë–∞–∑–æ–≤—ã–π URL API
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ—Å—Ç–∞—Ç–∫–∞–º–∏
        """
        headers = {"Authorization": api_token}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = "wb_stocks_today"
        cached_data = self.get_cached_data(
            CacheType.PRODUCT_INFO,
            endpoint="stocks",
            date=datetime.now().date().isoformat()
        )
        
        if cached_data:
            logger.info(f"üì¶ –ü–æ–ª—É—á–µ–Ω–æ {len(cached_data)} —Ç–æ–≤–∞—Ä–æ–≤ WB –∏–∑ –∫—ç—à–∞")
            return cached_data
        
        async with aiohttp.ClientSession() as session:
            url = f"{base_url}/api/v1/supplier/stocks"
            params = {
                'dateFrom': datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).isoformat()
            }
            
            response_data = await self.make_api_request(
                session=session,
                method='GET',
                url=url,
                marketplace='wb',
                headers=headers,
                params=params,
                timeout=30
            )
            
            if response_data and isinstance(response_data, list):
                # –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 30 –º–∏–Ω—É—Ç
                self.set_cached_data(
                    CacheType.PRODUCT_INFO,
                    response_data,
                    ttl_hours=0.5,
                    endpoint="stocks",
                    date=datetime.now().date().isoformat()
                )
                
                logger.info(f"üì¶ –ü–æ–ª—É—á–µ–Ω–æ {len(response_data)} —Ç–æ–≤–∞—Ä–æ–≤ —Å WB API")
                return response_data
            
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å WB API")
            return []
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        cache_hit_rate = 0.0
        total_cache_requests = self._stats['cache_hits'] + self._stats['cache_misses']
        
        if total_cache_requests > 0:
            cache_hit_rate = self._stats['cache_hits'] / total_cache_requests
        
        return {
            'cache_stats': {
                'hit_rate': cache_hit_rate,
                'total_entries': len(self._cache),
                'hits': self._stats['cache_hits'],
                'misses': self._stats['cache_misses']
            },
            'api_stats': {
                'total_requests': self._stats['api_requests'],
                'parallel_requests': self._stats['parallel_requests'],
                'batch_optimizations': self._stats['batch_optimizations']
            },
            'batch_configs': {
                api_type: asdict(config) for api_type, config in self._batch_configs.items()
            }
        }
    
    def cleanup(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞."""
        self._save_cache_to_disk()
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ API –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ API."""
    optimizer = APIRequestOptimizer()
    
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ Ozon
        ozon_stocks = await optimizer.fetch_ozon_stocks_optimized(
            client_id="your_client_id",
            api_key="your_api_key",
            base_url="https://api-seller.ozon.ru"
        )
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ WB
        wb_stocks = await optimizer.fetch_wb_stocks_optimized(
            api_token="your_token",
            base_url="https://suppliers-api.wildberries.ru"
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        stats = optimizer.get_performance_stats()
        print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {json.dumps(stats, indent=2, default=str)}")
        
    finally:
        optimizer.cleanup()


if __name__ == "__main__":
    asyncio.run(main())