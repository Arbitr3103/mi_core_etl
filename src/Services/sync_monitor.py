#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –∞–ª–µ—Ä—Ç–æ–≤ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤.

–ö–ª–∞—Å—Å SyncMonitor –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
–≤ –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤ –æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏.

–ê–≤—Ç–æ—Ä: ETL System
–î–∞—Ç–∞: 06 —è–Ω–≤–∞—Ä—è 2025
"""

import logging
import smtplib
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
try:
    from email.mime.text import MimeText
    from email.mime.multipart import MimeMultipart
except ImportError:
    # Email –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
    MimeText = None
    MimeMultipart = None
import statistics
import json


class HealthStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã."""
    HEALTHY = "healthy"
    WARNING = "warning"
    CRITICAL = "critical"
    UNKNOWN = "unknown"


class AnomalyType(Enum):
    """–¢–∏–ø—ã –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö."""
    ZERO_STOCK_SPIKE = "zero_stock_spike"
    MASSIVE_STOCK_CHANGE = "massive_stock_change"
    MISSING_PRODUCTS = "missing_products"
    DUPLICATE_RECORDS = "duplicate_records"
    NEGATIVE_STOCK = "negative_stock"
    STALE_DATA = "stale_data"
    API_ERRORS = "api_errors"


@dataclass
class Anomaly:
    """–ú–æ–¥–µ–ª—å –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö."""
    type: AnomalyType
    severity: str  # 'low', 'medium', 'high', 'critical'
    source: str
    description: str
    affected_records: int
    detected_at: datetime
    details: Dict[str, Any]


@dataclass
class HealthReport:
    """–û—Ç—á–µ—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏."""
    overall_status: HealthStatus
    generated_at: datetime
    sources: Dict[str, Dict[str, Any]]
    anomalies: List[Anomaly]
    recommendations: List[str]
    metrics: Dict[str, Any]


@dataclass
class SyncMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏."""
    source: str
    last_sync_time: Optional[datetime]
    success_rate_24h: float
    avg_duration_seconds: float
    total_records_processed: int
    error_count_24h: int
    data_freshness_hours: float


class SyncMonitor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤.
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –ü—Ä–æ–≤–µ—Ä–∫—É –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    - –î–µ—Ç–µ–∫—Ü–∏—é –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–æ–≤ –æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    - –û—Ç–ø—Ä–∞–≤–∫—É —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö
    """
    
    def __init__(self, db_cursor, db_connection, logger_name: str = "SyncMonitor"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏.
        
        Args:
            db_cursor: –ö—É—Ä—Å–æ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            db_connection: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
            logger_name: –ò–º—è –ª–æ–≥–≥–µ—Ä–∞
        """
        self.cursor = db_cursor
        self.connection = db_connection
        self.logger = logging.getLogger(logger_name)
        
        # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
        self.thresholds = {
            'data_freshness_hours': 6,  # –î–∞–Ω–Ω—ã–µ —Å—á–∏—Ç–∞—é—Ç—Å—è —É—Å—Ç–∞—Ä–µ–≤—à–∏–º–∏ —á–µ—Ä–µ–∑ 6 —á–∞—Å–æ–≤
            'success_rate_threshold': 0.8,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–π
            'massive_change_threshold': 0.5,  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 50%
            'zero_stock_threshold': 0.3,  # –ë–æ–ª–µ–µ 30% —Ç–æ–≤–∞—Ä–æ–≤ —Å –Ω—É–ª–µ–≤—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏
            'max_error_count_24h': 10,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –∑–∞ 24 —á–∞—Å–∞
        }
    
    def check_sync_health(self) -> HealthReport:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏.
        
        Returns:
            HealthReport: –û—Ç—á–µ—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã
        """
        self.logger.info("üîç –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            sources_metrics = self._get_sources_metrics()
            
            # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
            anomalies = self._detect_all_anomalies()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã
            overall_status = self._calculate_overall_health(sources_metrics, anomalies)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = self._generate_recommendations(sources_metrics, anomalies)
            
            # –°–æ–±–∏—Ä–∞–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            overall_metrics = self._calculate_overall_metrics(sources_metrics)
            
            report = HealthReport(
                overall_status=overall_status,
                generated_at=datetime.now(),
                sources=sources_metrics,
                anomalies=anomalies,
                recommendations=recommendations,
                metrics=overall_metrics
            )
            
            self.logger.info(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ç–∞—Ç—É—Å: {overall_status.value}")
            return report
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã: {e}")
            return HealthReport(
                overall_status=HealthStatus.UNKNOWN,
                generated_at=datetime.now(),
                sources={},
                anomalies=[],
                recommendations=[f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {str(e)}"],
                metrics={}
            )
    
    def detect_data_anomalies(self, source: Optional[str] = None) -> List[Anomaly]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤.
        
        Args:
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            List[Anomaly]: –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
        """
        self.logger.info(f"üîç –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source or '–≤—Å–µ'}")
        
        anomalies = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∞–Ω–æ–º–∞–ª–∏–π
            anomalies.extend(self._detect_zero_stock_anomalies(source))
            anomalies.extend(self._detect_massive_stock_changes(source))
            anomalies.extend(self._detect_missing_products(source))
            anomalies.extend(self._detect_duplicate_records(source))
            anomalies.extend(self._detect_negative_stock(source))
            anomalies.extend(self._detect_stale_data(source))
            anomalies.extend(self._detect_api_errors(source))
            
            self.logger.info(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(anomalies)} –∞–Ω–æ–º–∞–ª–∏–π")
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
            return []
    
    def generate_sync_report(self, period_hours: int = 24) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥.
        
        Args:
            period_hours: –ü–µ—Ä–∏–æ–¥ –¥–ª—è –æ—Ç—á–µ—Ç–∞ –≤ —á–∞—Å–∞—Ö
            
        Returns:
            Dict: –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        """
        self.logger.info(f"üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞ {period_hours} —á–∞—Å–æ–≤")
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ë–î –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
            if is_sqlite:
                sync_stats_query = """
                    SELECT source, status, COUNT(*) as count,
                           AVG(duration_seconds) as avg_duration,
                           SUM(records_processed) as total_processed,
                           SUM(records_inserted) as total_inserted,
                           SUM(records_failed) as total_failed,
                           MAX(completed_at) as last_sync
                    FROM sync_logs 
                    WHERE started_at >= datetime('now', '-{} hours')
                      AND sync_type = 'inventory'
                    GROUP BY source, status
                    ORDER BY source, status
                """.format(period_hours)
            else:
                sync_stats_query = """
                    SELECT source, status, COUNT(*) as count,
                           AVG(duration_seconds) as avg_duration,
                           SUM(records_processed) as total_processed,
                           SUM(records_inserted) as total_inserted,
                           SUM(records_failed) as total_failed,
                           MAX(completed_at) as last_sync
                    FROM sync_logs 
                    WHERE started_at >= DATE_SUB(NOW(), INTERVAL %s HOUR)
                      AND sync_type = 'inventory'
                    GROUP BY source, status
                    ORDER BY source, status
                """
            
            if is_sqlite:
                self.cursor.execute(sync_stats_query)
            else:
                self.cursor.execute(sync_stats_query, (period_hours,))
            
            sync_stats = self.cursor.fetchall()
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö
            inventory_stats_query = """
                SELECT source, COUNT(*) as total_products,
                       SUM(CASE WHEN current_stock > 0 THEN 1 ELSE 0 END) as products_with_stock,
                       SUM(current_stock) as total_stock,
                       AVG(current_stock) as avg_stock,
                       MAX(last_sync_at) as last_update
                FROM inventory_data 
                WHERE snapshot_date >= %s
                GROUP BY source
            """
            
            if is_sqlite:
                inventory_stats_query = inventory_stats_query.replace('%s', '?')
                self.cursor.execute(inventory_stats_query, (date.today() - timedelta(days=1),))
            else:
                self.cursor.execute(inventory_stats_query, (date.today() - timedelta(days=1),))
            
            inventory_stats = self.cursor.fetchall()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            report = {
                "generated_at": datetime.now(),
                "period_hours": period_hours,
                "sync_statistics": self._format_sync_statistics(sync_stats),
                "inventory_statistics": self._format_inventory_statistics(inventory_stats),
                "anomalies": [anomaly.__dict__ for anomaly in self.detect_data_anomalies()],
                "health_status": self.check_sync_health().__dict__
            }
            
            self.logger.info("üìä –û—Ç—á–µ—Ç –æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
            return report
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
            return {
                "error": str(e),
                "generated_at": datetime.now()
            }
    
    def _get_sources_metrics(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –¥–∞–Ω–Ω—ã—Ö."""
        sources_metrics = {}
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ë–î –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = ['Ozon', 'Wildberries']
            
            for source in sources:
                metrics = self._calculate_source_metrics(source)
                sources_metrics[source] = {
                    'last_sync_time': metrics.last_sync_time,
                    'success_rate_24h': metrics.success_rate_24h,
                    'avg_duration_seconds': metrics.avg_duration_seconds,
                    'total_records_processed': metrics.total_records_processed,
                    'error_count_24h': metrics.error_count_24h,
                    'data_freshness_hours': metrics.data_freshness_hours,
                    'health_status': self._determine_source_health(metrics)
                }
            
            return sources_metrics
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
            return {}
    
    def _calculate_source_metrics(self, source: str) -> SyncMetrics:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ë–î –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞ 24 —á–∞—Å–∞
            if is_sqlite:
                sync_query = """
                    SELECT status, COUNT(*) as count,
                           AVG(duration_seconds) as avg_duration,
                           SUM(records_processed) as total_processed,
                           MAX(completed_at) as last_sync
                    FROM sync_logs 
                    WHERE source = ? 
                      AND started_at >= datetime('now', '-24 hours')
                      AND sync_type = 'inventory'
                    GROUP BY status
                """
                self.cursor.execute(sync_query, (source,))
            else:
                sync_query = """
                    SELECT status, COUNT(*) as count,
                           AVG(duration_seconds) as avg_duration,
                           SUM(records_processed) as total_processed,
                           MAX(completed_at) as last_sync
                    FROM sync_logs 
                    WHERE source = %s 
                      AND started_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                      AND sync_type = 'inventory'
                    GROUP BY status
                """
                self.cursor.execute(sync_query, (source,))
            
            sync_results = self.cursor.fetchall()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            total_syncs = 0
            success_count = 0
            avg_duration = 0
            total_processed = 0
            last_sync_time = None
            error_count = 0
            
            for result in sync_results:
                if isinstance(result, dict):
                    status = result['status']
                    count = result['count']
                    duration = result['avg_duration'] or 0
                    processed = result['total_processed'] or 0
                    last_sync = result['last_sync']
                else:
                    status, count, duration, processed, last_sync = result
                
                total_syncs += count
                total_processed += processed or 0
                
                if status == 'success':
                    success_count += count
                elif status == 'failed':
                    error_count += count
                
                if duration and duration > avg_duration:
                    avg_duration = duration
                
                if last_sync and (not last_sync_time or last_sync > last_sync_time):
                    last_sync_time = last_sync
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            success_rate = success_count / total_syncs if total_syncs > 0 else 0
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–≤–µ–∂–µ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            data_freshness_hours = 999  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            if last_sync_time:
                if isinstance(last_sync_time, str):
                    last_sync_time = datetime.fromisoformat(last_sync_time.replace('Z', '+00:00'))
                data_freshness_hours = (datetime.now() - last_sync_time).total_seconds() / 3600
            
            return SyncMetrics(
                source=source,
                last_sync_time=last_sync_time,
                success_rate_24h=success_rate,
                avg_duration_seconds=avg_duration,
                total_records_processed=total_processed,
                error_count_24h=error_count,
                data_freshness_hours=data_freshness_hours
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è {source}: {e}")
            return SyncMetrics(
                source=source,
                last_sync_time=None,
                success_rate_24h=0,
                avg_duration_seconds=0,
                total_records_processed=0,
                error_count_24h=999,
                data_freshness_hours=999
            )
    
    def _determine_source_health(self, metrics: SyncMetrics) -> HealthStatus:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–¥–æ—Ä–æ–≤—å—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if (metrics.data_freshness_hours > 24 or 
            metrics.success_rate_24h < 0.5 or 
            metrics.error_count_24h > 20):
            return HealthStatus.CRITICAL
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if (metrics.data_freshness_hours > self.thresholds['data_freshness_hours'] or
            metrics.success_rate_24h < self.thresholds['success_rate_threshold'] or
            metrics.error_count_24h > self.thresholds['max_error_count_24h']):
            return HealthStatus.WARNING
        
        # –ó–¥–æ—Ä–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        if metrics.last_sync_time and metrics.success_rate_24h > 0.8:
            return HealthStatus.HEALTHY
        
        return HealthStatus.UNKNOWN
    
    def _detect_all_anomalies(self) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π."""
        anomalies = []
        
        for source in ['Ozon', 'Wildberries']:
            anomalies.extend(self.detect_data_anomalies(source))
        
        return anomalies
    
    def _detect_zero_stock_anomalies(self, source: Optional[str] = None) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π —Å –Ω—É–ª–µ–≤—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏."""
        anomalies = []
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ë–î –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            query = """
                SELECT source, 
                       COUNT(*) as total_products,
                       SUM(CASE WHEN current_stock = 0 THEN 1 ELSE 0 END) as zero_stock_count
                FROM inventory_data 
                WHERE snapshot_date = %s
            """
            
            params = [date.today()]
            
            if source:
                query += " AND source = %s"
                params.append(source)
            
            query += " GROUP BY source"
            
            if is_sqlite:
                query = query.replace('%s', '?')
            
            self.cursor.execute(query, params)
            results = self.cursor.fetchall()
            
            for result in results:
                if isinstance(result, dict):
                    src = result['source']
                    total = result['total_products']
                    zero_count = result['zero_stock_count']
                else:
                    src, total, zero_count = result
                
                if total > 0:
                    zero_ratio = zero_count / total
                    
                    if zero_ratio > self.thresholds['zero_stock_threshold']:
                        severity = 'critical' if zero_ratio > 0.7 else 'high' if zero_ratio > 0.5 else 'medium'
                        
                        anomaly = Anomaly(
                            type=AnomalyType.ZERO_STOCK_SPIKE,
                            severity=severity,
                            source=src,
                            description=f"–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Ç–æ–≤–∞—Ä–æ–≤ —Å –Ω—É–ª–µ–≤—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏: {zero_ratio:.1%}",
                            affected_records=zero_count,
                            detected_at=datetime.now(),
                            details={
                                'total_products': total,
                                'zero_stock_count': zero_count,
                                'zero_stock_ratio': zero_ratio,
                                'threshold': self.thresholds['zero_stock_threshold']
                            }
                        )
                        anomalies.append(anomaly)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –Ω—É–ª–µ–≤—ã—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤: {e}")
            return []
    
    def _detect_massive_stock_changes(self, source: Optional[str] = None) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –º–∞—Å—Å–æ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –æ—Å—Ç–∞—Ç–∫–æ–≤."""
        anomalies = []
        
        try:
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ—Å—Ç–∞—Ç–∫–∏ —Å–µ–≥–æ–¥–Ω—è –∏ –≤—á–µ—Ä–∞
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            query = """
                SELECT today.source, today.product_id, today.sku,
                       today.current_stock as today_stock,
                       yesterday.current_stock as yesterday_stock
                FROM inventory_data today
                LEFT JOIN inventory_data yesterday 
                    ON today.product_id = yesterday.product_id 
                    AND today.source = yesterday.source
                    AND yesterday.snapshot_date = %s
                WHERE today.snapshot_date = %s
            """
            
            params = [date.today() - timedelta(days=1), date.today()]
            
            if source:
                query += " AND today.source = %s"
                params.append(source)
            
            if is_sqlite:
                query = query.replace('%s', '?')
            
            self.cursor.execute(query, params)
            results = self.cursor.fetchall()
            
            massive_changes = []
            
            for result in results:
                if isinstance(result, dict):
                    src = result['source']
                    product_id = result['product_id']
                    sku = result['sku']
                    today_stock = result['today_stock'] or 0
                    yesterday_stock = result['yesterday_stock'] or 0
                else:
                    src, product_id, sku, today_stock, yesterday_stock = result
                    today_stock = today_stock or 0
                    yesterday_stock = yesterday_stock or 0
                
                if yesterday_stock > 0:
                    change_ratio = abs(today_stock - yesterday_stock) / yesterday_stock
                    
                    if change_ratio > self.thresholds['massive_change_threshold']:
                        massive_changes.append({
                            'source': src,
                            'product_id': product_id,
                            'sku': sku,
                            'change_ratio': change_ratio,
                            'today_stock': today_stock,
                            'yesterday_stock': yesterday_stock
                        })
            
            if massive_changes:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
                by_source = {}
                for change in massive_changes:
                    src = change['source']
                    if src not in by_source:
                        by_source[src] = []
                    by_source[src].append(change)
                
                for src, changes in by_source.items():
                    if len(changes) > 5:  # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å –º–∞—Å—Å–æ–≤—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
                        severity = 'critical' if len(changes) > 50 else 'high' if len(changes) > 20 else 'medium'
                        
                        anomaly = Anomaly(
                            type=AnomalyType.MASSIVE_STOCK_CHANGE,
                            severity=severity,
                            source=src,
                            description=f"–ú–∞—Å—Å–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤ —É {len(changes)} —Ç–æ–≤–∞—Ä–æ–≤",
                            affected_records=len(changes),
                            detected_at=datetime.now(),
                            details={
                                'affected_products': len(changes),
                                'threshold': self.thresholds['massive_change_threshold'],
                                'sample_changes': changes[:5]  # –ü–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                            }
                        )
                        anomalies.append(anomaly)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–∞—Å—Å–æ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π: {e}")
            return []
    
    def _detect_missing_products(self, source: Optional[str] = None) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤."""
        anomalies = []
        
        try:
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å–µ–≥–æ–¥–Ω—è –∏ –≤—á–µ—Ä–∞
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            query = """
                SELECT source,
                       COUNT(DISTINCT CASE WHEN snapshot_date = %s THEN product_id END) as today_count,
                       COUNT(DISTINCT CASE WHEN snapshot_date = %s THEN product_id END) as yesterday_count
                FROM inventory_data 
                WHERE snapshot_date IN (%s, %s)
            """
            
            today = date.today()
            yesterday = today - timedelta(days=1)
            params = [today, yesterday, today, yesterday]
            
            if source:
                query += " AND source = %s"
                params.append(source)
            
            query += " GROUP BY source"
            
            if is_sqlite:
                query = query.replace('%s', '?')
            
            self.cursor.execute(query, params)
            results = self.cursor.fetchall()
            
            for result in results:
                if isinstance(result, dict):
                    src = result['source']
                    today_count = result['today_count'] or 0
                    yesterday_count = result['yesterday_count'] or 0
                else:
                    src, today_count, yesterday_count = result
                    today_count = today_count or 0
                    yesterday_count = yesterday_count or 0
                
                if yesterday_count > 0:
                    missing_ratio = (yesterday_count - today_count) / yesterday_count
                    
                    if missing_ratio > 0.1:  # –ë–æ–ª–µ–µ 10% —Ç–æ–≤–∞—Ä–æ–≤ –∏—Å—á–µ–∑–ª–æ
                        severity = 'critical' if missing_ratio > 0.5 else 'high' if missing_ratio > 0.3 else 'medium'
                        
                        anomaly = Anomaly(
                            type=AnomalyType.MISSING_PRODUCTS,
                            severity=severity,
                            source=src,
                            description=f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {yesterday_count - today_count} —Ç–æ–≤–∞—Ä–æ–≤ ({missing_ratio:.1%})",
                            affected_records=yesterday_count - today_count,
                            detected_at=datetime.now(),
                            details={
                                'today_count': today_count,
                                'yesterday_count': yesterday_count,
                                'missing_count': yesterday_count - today_count,
                                'missing_ratio': missing_ratio
                            }
                        )
                        anomalies.append(anomaly)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤: {e}")
            return []
    
    def _detect_duplicate_records(self, source: Optional[str] = None) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π."""
        anomalies = []
        
        try:
            # –ò—â–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ product_id, source, snapshot_date
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            query = """
                SELECT source, product_id, snapshot_date, COUNT(*) as duplicate_count
                FROM inventory_data 
                WHERE snapshot_date = %s
            """
            
            params = [date.today()]
            
            if source:
                query += " AND source = %s"
                params.append(source)
            
            query += """
                GROUP BY source, product_id, snapshot_date
                HAVING COUNT(*) > 1
            """
            
            if is_sqlite:
                query = query.replace('%s', '?')
            
            self.cursor.execute(query, params)
            results = self.cursor.fetchall()
            
            if results:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
                by_source = {}
                total_duplicates = 0
                
                for result in results:
                    if isinstance(result, dict):
                        src = result['source']
                        duplicate_count = result['duplicate_count']
                    else:
                        src, product_id, snapshot_date, duplicate_count = result
                    
                    if src not in by_source:
                        by_source[src] = 0
                    by_source[src] += duplicate_count - 1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—à–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π
                    total_duplicates += duplicate_count - 1
                
                for src, dup_count in by_source.items():
                    if dup_count > 0:
                        severity = 'high' if dup_count > 100 else 'medium' if dup_count > 10 else 'low'
                        
                        anomaly = Anomaly(
                            type=AnomalyType.DUPLICATE_RECORDS,
                            severity=severity,
                            source=src,
                            description=f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {dup_count} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π",
                            affected_records=dup_count,
                            detected_at=datetime.now(),
                            details={
                                'duplicate_count': dup_count,
                                'total_duplicates': total_duplicates
                            }
                        )
                        anomalies.append(anomaly)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
            return []
    
    def _detect_negative_stock(self, source: Optional[str] = None) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤."""
        anomalies = []
        
        try:
            # –ò—â–µ–º –∑–∞–ø–∏—Å–∏ —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            query = """
                SELECT source, COUNT(*) as negative_count
                FROM inventory_data 
                WHERE snapshot_date = %s
                  AND (current_stock < 0 OR available_stock < 0 OR quantity_present < 0)
            """
            
            params = [date.today()]
            
            if source:
                query += " AND source = %s"
                params.append(source)
            
            query += " GROUP BY source"
            
            if is_sqlite:
                query = query.replace('%s', '?')
            
            self.cursor.execute(query, params)
            results = self.cursor.fetchall()
            
            for result in results:
                if isinstance(result, dict):
                    src = result['source']
                    negative_count = result['negative_count']
                else:
                    src, negative_count = result
                
                if negative_count > 0:
                    severity = 'critical' if negative_count > 50 else 'high' if negative_count > 10 else 'medium'
                    
                    anomaly = Anomaly(
                        type=AnomalyType.NEGATIVE_STOCK,
                        severity=severity,
                        source=src,
                        description=f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {negative_count} –∑–∞–ø–∏—Å–µ–π —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏",
                        affected_records=negative_count,
                        detected_at=datetime.now(),
                        details={
                            'negative_count': negative_count
                        }
                    )
                    anomalies.append(anomaly)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤: {e}")
            return []
    
    def _detect_stale_data(self, source: Optional[str] = None) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""
        anomalies = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            query = """
                SELECT source, MAX(last_sync_at) as last_update
                FROM inventory_data 
                WHERE 1=1
            """
            
            params = []
            
            if source:
                query += " AND source = %s"
                params.append(source)
            
            query += " GROUP BY source"
            
            if is_sqlite:
                query = query.replace('%s', '?')
            
            self.cursor.execute(query, params)
            results = self.cursor.fetchall()
            
            for result in results:
                if isinstance(result, dict):
                    src = result['source']
                    last_update = result['last_update']
                else:
                    src, last_update = result
                
                if last_update:
                    if isinstance(last_update, str):
                        last_update = datetime.fromisoformat(last_update.replace('Z', '+00:00'))
                    
                    hours_since_update = (datetime.now() - last_update).total_seconds() / 3600
                    
                    if hours_since_update > self.thresholds['data_freshness_hours']:
                        severity = 'critical' if hours_since_update > 24 else 'high' if hours_since_update > 12 else 'medium'
                        
                        anomaly = Anomaly(
                            type=AnomalyType.STALE_DATA,
                            severity=severity,
                            source=src,
                            description=f"–î–∞–Ω–Ω—ã–µ –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å {hours_since_update:.1f} —á–∞—Å–æ–≤",
                            affected_records=0,
                            detected_at=datetime.now(),
                            details={
                                'last_update': last_update,
                                'hours_since_update': hours_since_update,
                                'threshold_hours': self.thresholds['data_freshness_hours']
                            }
                        )
                        anomalies.append(anomaly)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return []
    
    def _detect_api_errors(self, source: Optional[str] = None) -> List[Anomaly]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –æ—à–∏–±–æ–∫ API."""
        anomalies = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—à–∏–±–∫–∏ –≤ –ª–æ–≥–∞—Ö —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
            is_sqlite = hasattr(self.cursor, 'lastrowid') and 'sqlite' in str(type(self.cursor)).lower()
            
            if is_sqlite:
                query = """
                    SELECT source, COUNT(*) as error_count,
                           GROUP_CONCAT(DISTINCT error_message) as error_messages
                    FROM sync_logs 
                    WHERE status = 'failed'
                      AND started_at >= datetime('now', '-24 hours')
                      AND sync_type = 'inventory'
                """
            else:
                query = """
                    SELECT source, COUNT(*) as error_count,
                           GROUP_CONCAT(DISTINCT error_message SEPARATOR '; ') as error_messages
                    FROM sync_logs 
                    WHERE status = 'failed'
                      AND started_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                      AND sync_type = 'inventory'
                """
            
            params = []
            
            if source:
                query += " AND source = %s"
                params.append(source)
            
            query += " GROUP BY source"
            
            if is_sqlite:
                query = query.replace('%s', '?')
            
            self.cursor.execute(query, params)
            results = self.cursor.fetchall()
            
            for result in results:
                if isinstance(result, dict):
                    src = result['source']
                    error_count = result['error_count']
                    error_messages = result['error_messages']
                else:
                    src, error_count, error_messages = result
                
                if error_count > self.thresholds['max_error_count_24h']:
                    severity = 'critical' if error_count > 50 else 'high' if error_count > 20 else 'medium'
                    
                    anomaly = Anomaly(
                        type=AnomalyType.API_ERRORS,
                        severity=severity,
                        source=src,
                        description=f"–í—ã—Å–æ–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ API: {error_count} –∑–∞ 24 —á–∞—Å–∞",
                        affected_records=error_count,
                        detected_at=datetime.now(),
                        details={
                            'error_count_24h': error_count,
                            'threshold': self.thresholds['max_error_count_24h'],
                            'sample_errors': error_messages[:500] if error_messages else None  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                        }
                    )
                    anomalies.append(anomaly)
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ—à–∏–±–æ–∫ API: {e}")
            return []
    
    def _calculate_overall_health(self, sources_metrics: Dict[str, Dict[str, Any]], 
                                anomalies: List[Anomaly]) -> HealthStatus:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏
        critical_anomalies = [a for a in anomalies if a.severity == 'critical']
        if critical_anomalies:
            return HealthStatus.CRITICAL
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        critical_sources = [s for s in sources_metrics.values() 
                          if s.get('health_status') == HealthStatus.CRITICAL]
        if critical_sources:
            return HealthStatus.CRITICAL
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        warning_anomalies = [a for a in anomalies if a.severity in ['high', 'medium']]
        warning_sources = [s for s in sources_metrics.values() 
                         if s.get('health_status') == HealthStatus.WARNING]
        
        if warning_anomalies or warning_sources:
            return HealthStatus.WARNING
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–¥–æ—Ä–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        healthy_sources = [s for s in sources_metrics.values() 
                         if s.get('health_status') == HealthStatus.HEALTHY]
        
        if len(healthy_sources) == len(sources_metrics):
            return HealthStatus.HEALTHY
        
        return HealthStatus.UNKNOWN
    
    def _generate_recommendations(self, sources_metrics: Dict[str, Dict[str, Any]], 
                                anomalies: List[Anomaly]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–∏—Å—Ç–µ–º—ã."""
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–Ω–æ–º–∞–ª–∏—è–º
        for anomaly in anomalies:
            if anomaly.type == AnomalyType.STALE_DATA:
                recommendations.append(f"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è {anomaly.source}")
            elif anomaly.type == AnomalyType.API_ERRORS:
                recommendations.append(f"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å API –∫–ª—é—á–∏ –∏ –ª–∏–º–∏—Ç—ã –¥–ª—è {anomaly.source}")
            elif anomaly.type == AnomalyType.ZERO_STOCK_SPIKE:
                recommendations.append(f"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö –≤ {anomaly.source}")
            elif anomaly.type == AnomalyType.DUPLICATE_RECORDS:
                recommendations.append(f"–û—á–∏—Å—Ç–∏—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∑–∞–ø–∏—Å–∏ –≤ {anomaly.source}")
            elif anomaly.type == AnomalyType.NEGATIVE_STOCK:
                recommendations.append(f"–ò—Å–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å–∏ —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏ –≤ {anomaly.source}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        for source, metrics in sources_metrics.items():
            if metrics.get('success_rate_24h', 0) < 0.8:
                recommendations.append(f"–£–ª—É—á—à–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è {source}")
            
            if metrics.get('data_freshness_hours', 0) > 6:
                recommendations.append(f"–£–≤–µ–ª–∏—á–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è {source}")
            
            if metrics.get('error_count_24h', 0) > 10:
                recommendations.append(f"–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–∏—á–∏–Ω—ã –æ—à–∏–±–æ–∫ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è {source}")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if not recommendations:
            recommendations.append("–°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥.")
        
        return recommendations[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    
    def _calculate_overall_metrics(self, sources_metrics: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫ —Å–∏—Å—Ç–µ–º—ã."""
        if not sources_metrics:
            return {}
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        success_rates = [m.get('success_rate_24h', 0) for m in sources_metrics.values()]
        durations = [m.get('avg_duration_seconds', 0) for m in sources_metrics.values() if m.get('avg_duration_seconds', 0) > 0]
        total_processed = sum(m.get('total_records_processed', 0) for m in sources_metrics.values())
        total_errors = sum(m.get('error_count_24h', 0) for m in sources_metrics.values())
        
        return {
            'avg_success_rate': statistics.mean(success_rates) if success_rates else 0,
            'avg_duration_seconds': statistics.mean(durations) if durations else 0,
            'total_records_processed_24h': total_processed,
            'total_errors_24h': total_errors,
            'active_sources': len(sources_metrics),
            'healthy_sources': len([m for m in sources_metrics.values() 
                                  if m.get('health_status') == HealthStatus.HEALTHY])
        }
    
    def _format_sync_statistics(self, sync_stats) -> Dict[str, Any]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏."""
        formatted = {}
        
        for stat in sync_stats:
            if isinstance(stat, dict):
                source = stat['source']
                status = stat['status']
                count = stat['count']
                avg_duration = stat['avg_duration']
                total_processed = stat['total_processed']
                total_inserted = stat['total_inserted']
                total_failed = stat['total_failed']
                last_sync = stat['last_sync']
            else:
                source, status, count, avg_duration, total_processed, total_inserted, total_failed, last_sync = stat
            
            if source not in formatted:
                formatted[source] = {}
            
            formatted[source][status] = {
                'count': count,
                'avg_duration': avg_duration,
                'total_processed': total_processed,
                'total_inserted': total_inserted,
                'total_failed': total_failed,
                'last_sync': last_sync
            }
        
        return formatted
    
    def _format_inventory_statistics(self, inventory_stats) -> Dict[str, Any]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤."""
        formatted = {}
        
        for stat in inventory_stats:
            if isinstance(stat, dict):
                source = stat['source']
                total_products = stat['total_products']
                products_with_stock = stat['products_with_stock']
                total_stock = stat['total_stock']
                avg_stock = stat['avg_stock']
                last_update = stat['last_update']
            else:
                source, total_products, products_with_stock, total_stock, avg_stock, last_update = stat
            
            formatted[source] = {
                'total_products': total_products,
                'products_with_stock': products_with_stock,
                'products_without_stock': total_products - products_with_stock,
                'total_stock': total_stock,
                'avg_stock': avg_stock,
                'last_update': last_update
            }
        
        return formatted


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è SyncMonitor
    import mysql.connector
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î (–ø—Ä–∏–º–µ—Ä)
    try:
        connection = mysql.connector.connect(
            host='localhost',
            database='test_db',
            user='test_user',
            password='test_password'
        )
        cursor = connection.cursor(dictionary=True)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∞
        monitor = SyncMonitor(cursor, connection)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
        health_report = monitor.check_sync_health()
        print(f"–û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã: {health_report.overall_status.value}")
        print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(health_report.anomalies)}")
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        ozon_anomalies = monitor.detect_data_anomalies("Ozon")
        print(f"–ê–Ω–æ–º–∞–ª–∏–∏ Ozon: {len(ozon_anomalies)}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        sync_report = monitor.generate_sync_report(24)
        print(f"–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {sync_report.get('generated_at')}")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'connection' in locals():
            connection.close()