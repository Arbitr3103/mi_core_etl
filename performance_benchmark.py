#!/usr/bin/env python3
"""
–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤.

–ò–∑–º–µ—Ä—è–µ—Ç:
- –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- –ü—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

–ê–≤—Ç–æ—Ä: ETL System
–î–∞—Ç–∞: 06 —è–Ω–≤–∞—Ä—è 2025
"""

import os
import sys
import time
import psutil
import threading
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import logging
import json
import tempfile
import shutil
from contextlib import contextmanager
import tracemalloc
import gc

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

try:
    from inventory_sync_service_optimized import OptimizedInventorySyncService, InventoryRecord
    from api_request_optimizer import APIRequestOptimizer, CacheType
    from parallel_sync_manager import ParallelSyncManager
    from load_test_inventory_sync import MockDataGenerator, ResourceMonitor
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    operation_name: str
    dataset_size: int
    iterations: int
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_time_seconds: float
    avg_time_seconds: float
    min_time_seconds: float
    max_time_seconds: float
    median_time_seconds: float
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    throughput_per_second: float
    operations_per_second: float
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
    peak_memory_mb: float
    avg_cpu_percent: float
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    cache_hit_rate: float = 0.0
    error_rate: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å."""
        return asdict(self)


class PerformanceBenchmark:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
    
    def __init__(self):
        self.temp_dir = tempfile.mkdtemp()
        self.results: List[BenchmarkResult] = []
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    @contextmanager
    def measure_performance(self, operation_name: str):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
        monitor = ResourceMonitor(interval_seconds=0.1)
        monitor.start()
        
        # –í–∫–ª—é—á–∞–µ–º —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É –ø–∞–º—è—Ç–∏
        tracemalloc.start()
        
        start_time = time.perf_counter()
        
        try:
            yield
        finally:
            end_time = time.perf_counter()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            monitor.stop()
            resource_summary = monitor.get_summary()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._last_measurement = {
                'duration': end_time - start_time,
                'peak_memory_mb': peak / 1024 / 1024,
                'avg_cpu_percent': resource_summary.get('avg_cpu_percent', 0),
                'resource_summary': resource_summary
            }
    
    def benchmark_data_processing(self, dataset_sizes: List[int], iterations: int = 5) -> List[BenchmarkResult]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        
        results = []
        
        for dataset_size in dataset_sizes:
            logger.info(f"üìä –ë–µ–Ω—á–º–∞—Ä–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {dataset_size} –∑–∞–ø–∏—Å–µ–π")
            
            times = []
            memory_peaks = []
            cpu_averages = []
            
            for iteration in range(iterations):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                test_data = MockDataGenerator.generate_ozon_inventory_data(dataset_size)
                
                # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å
                service = OptimizedInventorySyncService(batch_size=1000, max_workers=4)
                service.product_cache.get_product_id_by_ozon_sku = lambda x: 1  # –ú–æ–∫ –∫—ç—à–∞
                
                with self.measure_performance(f"data_processing_{dataset_size}"):
                    processed_records = service.process_inventory_batch(test_data, 'Ozon')
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                measurement = self._last_measurement
                times.append(measurement['duration'])
                memory_peaks.append(measurement['peak_memory_mb'])
                cpu_averages.append(measurement['avg_cpu_percent'])
                
                # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É –∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏
                del test_data, processed_records
                gc.collect()
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞
            result = BenchmarkResult(
                operation_name="data_processing",
                dataset_size=dataset_size,
                iterations=iterations,
                total_time_seconds=sum(times),
                avg_time_seconds=statistics.mean(times),
                min_time_seconds=min(times),
                max_time_seconds=max(times),
                median_time_seconds=statistics.median(times),
                throughput_per_second=dataset_size / statistics.mean(times),
                operations_per_second=1 / statistics.mean(times),
                peak_memory_mb=max(memory_peaks),
                avg_cpu_percent=statistics.mean(cpu_averages)
            )
            
            results.append(result)
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ {dataset_size} –∑–∞–ø–∏—Å–µ–π: {result.throughput_per_second:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫, "
                       f"–ø–∞–º—è—Ç—å: {result.peak_memory_mb:.1f} –ú–ë")
        
        return results
    
    def benchmark_cache_performance(self, cache_sizes: List[int], iterations: int = 10) -> List[BenchmarkResult]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        results = []
        
        for cache_size in cache_sizes:
            logger.info(f"üìä –ë–µ–Ω—á–º–∞—Ä–∫ –∫—ç—à–∞ —Ä–∞–∑–º–µ—Ä–æ–º {cache_size}")
            
            # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä API —Å –∫—ç—à–µ–º
            optimizer = APIRequestOptimizer(cache_dir=self.temp_dir, max_cache_size=cache_size)
            
            times = []
            hit_rates = []
            
            for iteration in range(iterations):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
                test_data = {
                    "products": [
                        {"id": i, "name": f"Product {i}", "price": i * 10}
                        for i in range(cache_size)
                    ]
                }
                
                with self.measure_performance(f"cache_operations_{cache_size}"):
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
                    for i in range(cache_size):
                        optimizer.set_cached_data(
                            CacheType.PRODUCT_INFO,
                            {"product": test_data["products"][i]},
                            endpoint=f"product_{i}"
                        )
                    
                    # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞
                    cache_hits = 0
                    for i in range(cache_size):
                        cached_data = optimizer.get_cached_data(
                            CacheType.PRODUCT_INFO,
                            endpoint=f"product_{i}"
                        )
                        if cached_data:
                            cache_hits += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                measurement = self._last_measurement
                times.append(measurement['duration'])
                hit_rates.append(cache_hits / cache_size)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞
            result = BenchmarkResult(
                operation_name="cache_operations",
                dataset_size=cache_size,
                iterations=iterations,
                total_time_seconds=sum(times),
                avg_time_seconds=statistics.mean(times),
                min_time_seconds=min(times),
                max_time_seconds=max(times),
                median_time_seconds=statistics.median(times),
                throughput_per_second=cache_size * 2 / statistics.mean(times),  # read + write
                operations_per_second=1 / statistics.mean(times),
                peak_memory_mb=self._last_measurement['peak_memory_mb'],
                avg_cpu_percent=statistics.mean([self._last_measurement['avg_cpu_percent']]),
                cache_hit_rate=statistics.mean(hit_rates)
            )
            
            results.append(result)
            
            logger.info(f"‚úÖ –ö—ç—à {cache_size} –∑–∞–ø–∏—Å–µ–π: {result.throughput_per_second:.1f} –æ–ø–µ—Ä–∞—Ü–∏–π/—Å–µ–∫, "
                       f"hit rate: {result.cache_hit_rate:.2%}")
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            optimizer.cleanup()
        
        return results
    
    def benchmark_batch_sizes(self, dataset_size: int, batch_sizes: List[int], iterations: int = 3) -> List[BenchmarkResult]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π")
        
        results = []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω —Ä–∞–∑
        test_data = MockDataGenerator.generate_ozon_inventory_data(dataset_size)
        
        for batch_size in batch_sizes:
            logger.info(f"üìä –ë–µ–Ω—á–º–∞—Ä–∫ –±–∞—Ç—á–∞ —Ä–∞–∑–º–µ—Ä–æ–º {batch_size}")
            
            times = []
            memory_peaks = []
            cpu_averages = []
            
            for iteration in range(iterations):
                # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞
                service = OptimizedInventorySyncService(batch_size=batch_size, max_workers=4)
                service.product_cache.get_product_id_by_ozon_sku = lambda x: 1
                
                with self.measure_performance(f"batch_processing_{batch_size}"):
                    processed_records = service.process_inventory_batch(test_data, 'Ozon')
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                measurement = self._last_measurement
                times.append(measurement['duration'])
                memory_peaks.append(measurement['peak_memory_mb'])
                cpu_averages.append(measurement['avg_cpu_percent'])
                
                del processed_records
                gc.collect()
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞
            result = BenchmarkResult(
                operation_name="batch_processing",
                dataset_size=batch_size,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º batch_size –∫–∞–∫ dataset_size –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
                iterations=iterations,
                total_time_seconds=sum(times),
                avg_time_seconds=statistics.mean(times),
                min_time_seconds=min(times),
                max_time_seconds=max(times),
                median_time_seconds=statistics.median(times),
                throughput_per_second=dataset_size / statistics.mean(times),
                operations_per_second=1 / statistics.mean(times),
                peak_memory_mb=max(memory_peaks),
                avg_cpu_percent=statistics.mean(cpu_averages)
            )
            
            results.append(result)
            
            logger.info(f"‚úÖ –ë–∞—Ç—á {batch_size}: {result.throughput_per_second:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫, "
                       f"–≤—Ä–µ–º—è: {result.avg_time_seconds:.2f}—Å")
        
        return results
    
    def benchmark_parallel_workers(self, dataset_size: int, worker_counts: List[int], iterations: int = 3) -> List[BenchmarkResult]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤")
        
        results = []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω —Ä–∞–∑
        test_data = MockDataGenerator.generate_ozon_inventory_data(dataset_size)
        
        for workers in worker_counts:
            logger.info(f"üìä –ë–µ–Ω—á–º–∞—Ä–∫ —Å {workers} –≤–æ—Ä–∫–µ—Ä–∞–º–∏")
            
            times = []
            memory_peaks = []
            cpu_averages = []
            
            for iteration in range(iterations):
                # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤–æ—Ä–∫–µ—Ä–æ–≤
                service = OptimizedInventorySyncService(batch_size=1000, max_workers=workers)
                service.product_cache.get_product_id_by_ozon_sku = lambda x: 1
                
                with self.measure_performance(f"parallel_workers_{workers}"):
                    processed_records = service.process_inventory_batch(test_data, 'Ozon')
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                measurement = self._last_measurement
                times.append(measurement['duration'])
                memory_peaks.append(measurement['peak_memory_mb'])
                cpu_averages.append(measurement['avg_cpu_percent'])
                
                del processed_records
                gc.collect()
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞
            result = BenchmarkResult(
                operation_name="parallel_workers",
                dataset_size=workers,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º workers –∫–∞–∫ dataset_size –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
                iterations=iterations,
                total_time_seconds=sum(times),
                avg_time_seconds=statistics.mean(times),
                min_time_seconds=min(times),
                max_time_seconds=max(times),
                median_time_seconds=statistics.median(times),
                throughput_per_second=dataset_size / statistics.mean(times),
                operations_per_second=1 / statistics.mean(times),
                peak_memory_mb=max(memory_peaks),
                avg_cpu_percent=statistics.mean(cpu_averages)
            )
            
            results.append(result)
            
            logger.info(f"‚úÖ {workers} –≤–æ—Ä–∫–µ—Ä–æ–≤: {result.throughput_per_second:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫, "
                       f"CPU: {result.avg_cpu_percent:.1f}%")
        
        return results
    
    def benchmark_memory_efficiency(self, dataset_sizes: List[int]) -> List[BenchmarkResult]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏")
        
        results = []
        
        for dataset_size in dataset_sizes:
            logger.info(f"üìä –ë–µ–Ω—á–º–∞—Ä–∫ –ø–∞–º—è—Ç–∏ –¥–ª—è {dataset_size} –∑–∞–ø–∏—Å–µ–π")
            
            # –ò–∑–º–µ—Ä—è–µ–º –±–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
            gc.collect()
            process = psutil.Process()
            baseline_memory = process.memory_info().rss / 1024 / 1024
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –∏–∑–º–µ—Ä—è–µ–º –ø–∏–∫–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            with self.measure_performance(f"memory_efficiency_{dataset_size}"):
                test_data = MockDataGenerator.generate_ozon_inventory_data(dataset_size)
                
                service = OptimizedInventorySyncService(batch_size=1000, max_workers=4)
                service.product_cache.get_product_id_by_ozon_sku = lambda x: 1
                
                processed_records = service.process_inventory_batch(test_data, 'Ozon')
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è
                del test_data, processed_records
                gc.collect()
            
            measurement = self._last_measurement
            memory_overhead = measurement['peak_memory_mb'] - baseline_memory
            memory_per_record = memory_overhead / dataset_size * 1024  # –ö–ë –Ω–∞ –∑–∞–ø–∏—Å—å
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞
            result = BenchmarkResult(
                operation_name="memory_efficiency",
                dataset_size=dataset_size,
                iterations=1,
                total_time_seconds=measurement['duration'],
                avg_time_seconds=measurement['duration'],
                min_time_seconds=measurement['duration'],
                max_time_seconds=measurement['duration'],
                median_time_seconds=measurement['duration'],
                throughput_per_second=dataset_size / measurement['duration'],
                operations_per_second=1 / measurement['duration'],
                peak_memory_mb=measurement['peak_memory_mb'],
                avg_cpu_percent=measurement['avg_cpu_percent']
            )
            
            results.append(result)
            
            logger.info(f"‚úÖ –ü–∞–º—è—Ç—å –¥–ª—è {dataset_size} –∑–∞–ø–∏—Å–µ–π: {memory_overhead:.1f} –ú–ë "
                       f"({memory_per_record:.2f} –ö–ë/–∑–∞–ø–∏—Å—å)")
        
        return results
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        
        start_time = datetime.now()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
        dataset_sizes = [1000, 5000, 10000, 25000]
        cache_sizes = [100, 500, 1000, 5000]
        batch_sizes = [100, 500, 1000, 2000, 5000]
        worker_counts = [1, 2, 4, 8]
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
        benchmark_results = {}
        
        try:
            # –ë–µ–Ω—á–º–∞—Ä–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
            logger.info("=" * 50)
            logger.info("–ë–ï–ù–ß–ú–ê–†–ö –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•")
            logger.info("=" * 50)
            benchmark_results['data_processing'] = self.benchmark_data_processing(dataset_sizes)
            
            # –ë–µ–Ω—á–º–∞—Ä–∫ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
            logger.info("=" * 50)
            logger.info("–ë–ï–ù–ß–ú–ê–†–ö –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø")
            logger.info("=" * 50)
            benchmark_results['cache_performance'] = self.benchmark_cache_performance(cache_sizes)
            
            # –ë–µ–Ω—á–º–∞—Ä–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π
            logger.info("=" * 50)
            logger.info("–ë–ï–ù–ß–ú–ê–†–ö –†–ê–ó–ú–ï–†–û–í –ë–ê–¢–ß–ï–ô")
            logger.info("=" * 50)
            benchmark_results['batch_sizes'] = self.benchmark_batch_sizes(10000, batch_sizes)
            
            # –ë–µ–Ω—á–º–∞—Ä–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
            logger.info("=" * 50)
            logger.info("–ë–ï–ù–ß–ú–ê–†–ö –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–• –í–û–†–ö–ï–†–û–í")
            logger.info("=" * 50)
            benchmark_results['parallel_workers'] = self.benchmark_parallel_workers(10000, worker_counts)
            
            # –ë–µ–Ω—á–º–∞—Ä–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏
            logger.info("=" * 50)
            logger.info("–ë–ï–ù–ß–ú–ê–†–ö –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò –ü–ê–ú–Ø–¢–ò")
            logger.info("=" * 50)
            benchmark_results['memory_efficiency'] = self.benchmark_memory_efficiency(dataset_sizes)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
            raise
        
        end_time = datetime.now()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analysis = self._analyze_benchmark_results(benchmark_results)
        
        # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report = {
            'benchmark_summary': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_duration_minutes': (end_time - start_time).total_seconds() / 60,
                'system_info': {
                    'cpu_count': psutil.cpu_count(),
                    'memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
                    'platform': sys.platform
                }
            },
            'benchmark_results': {
                category: [result.to_dict() for result in results]
                for category, results in benchmark_results.items()
            },
            'performance_analysis': analysis,
            'optimal_configurations': self._find_optimal_configurations(benchmark_results)
        }
        
        logger.info("‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω")
        return report
    
    def _analyze_benchmark_results(self, results: Dict[str, List[BenchmarkResult]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        analysis = {}
        
        for category, benchmark_results in results.items():
            if not benchmark_results:
                continue
            
            throughputs = [r.throughput_per_second for r in benchmark_results]
            memory_usage = [r.peak_memory_mb for r in benchmark_results]
            
            analysis[category] = {
                'best_throughput': max(throughputs),
                'worst_throughput': min(throughputs),
                'avg_throughput': statistics.mean(throughputs),
                'throughput_variance': statistics.variance(throughputs) if len(throughputs) > 1 else 0,
                'memory_efficiency': {
                    'min_memory_mb': min(memory_usage),
                    'max_memory_mb': max(memory_usage),
                    'avg_memory_mb': statistics.mean(memory_usage)
                },
                'scalability_factor': self._calculate_scalability_factor(benchmark_results)
            }
        
        return analysis
    
    def _calculate_scalability_factor(self, results: List[BenchmarkResult]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏."""
        if len(results) < 2:
            return 1.0
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
        sorted_results = sorted(results, key=lambda r: r.dataset_size)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞–∑–º–µ—Ä—É –¥–∞–Ω–Ω—ã—Ö
        ratios = []
        for i in range(1, len(sorted_results)):
            prev_result = sorted_results[i-1]
            curr_result = sorted_results[i]
            
            size_ratio = curr_result.dataset_size / prev_result.dataset_size
            throughput_ratio = curr_result.throughput_per_second / prev_result.throughput_per_second
            
            # –ò–¥–µ–∞–ª—å–Ω–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å = 1.0 (–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä—É)
            scalability = throughput_ratio / size_ratio
            ratios.append(scalability)
        
        return statistics.mean(ratios) if ratios else 1.0
    
    def _find_optimal_configurations(self, results: Dict[str, List[BenchmarkResult]]) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π."""
        optimal = {}
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        if 'batch_sizes' in results:
            batch_results = results['batch_sizes']
            best_batch = max(batch_results, key=lambda r: r.throughput_per_second)
            optimal['batch_size'] = {
                'size': best_batch.dataset_size,
                'throughput': best_batch.throughput_per_second,
                'memory_mb': best_batch.peak_memory_mb
            }
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
        if 'parallel_workers' in results:
            worker_results = results['parallel_workers']
            # –ò—â–µ–º –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ—Å—É—Ä—Å–æ–≤
            best_worker = max(worker_results, 
                            key=lambda r: r.throughput_per_second / (r.avg_cpu_percent / 100 + 0.1))
            optimal['worker_count'] = {
                'count': best_worker.dataset_size,
                'throughput': best_worker.throughput_per_second,
                'cpu_percent': best_worker.avg_cpu_percent
            }
        
        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        if 'cache_performance' in results:
            cache_results = results['cache_performance']
            best_cache = max(cache_results, key=lambda r: r.cache_hit_rate * r.throughput_per_second)
            optimal['cache_size'] = {
                'size': best_cache.dataset_size,
                'hit_rate': best_cache.cache_hit_rate,
                'throughput': best_cache.throughput_per_second
            }
        
        return optimal


def save_benchmark_report(report: Dict[str, Any], filename: str = None):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_benchmark_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    logger.info(f"üìÑ –û—Ç—á–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")


def print_benchmark_summary(report: Dict[str, Any]):
    """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    print("\n" + "="*80)
    print("–°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ë–ï–ù–ß–ú–ê–†–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
    print("="*80)
    
    summary = report['benchmark_summary']
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {summary['total_duration_minutes']:.1f} –º–∏–Ω—É—Ç")
    print(f"–°–∏—Å—Ç–µ–º–∞: {summary['system_info']['cpu_count']} CPU, {summary['system_info']['memory_gb']:.1f} –ì–ë RAM")
    
    print("\n–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ï–ù–ß–ú–ê–†–ö–û–í:")
    print("-"*40)
    
    analysis = report['performance_analysis']
    for category, stats in analysis.items():
        print(f"\n{category.upper().replace('_', ' ')}:")
        print(f"  –õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {stats['best_throughput']:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
        print(f"  –°—Ä–µ–¥–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {stats['avg_throughput']:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
        print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {stats['memory_efficiency']['avg_memory_mb']:.1f} –ú–ë")
        print(f"  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏: {stats['scalability_factor']:.2f}")
    
    print("\n–û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò:")
    print("-"*40)
    optimal = report['optimal_configurations']
    
    if 'batch_size' in optimal:
        batch = optimal['batch_size']
        print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch['size']} ({batch['throughput']:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫)")
    
    if 'worker_count' in optimal:
        workers = optimal['worker_count']
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤: {workers['count']} ({workers['throughput']:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫)")
    
    if 'cache_size' in optimal:
        cache = optimal['cache_size']
        print(f"–†–∞–∑–º–µ—Ä –∫—ç—à–∞: {cache['size']} (hit rate: {cache['hit_rate']:.2%})")
    
    print("\n" + "="*80)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
    
    benchmark = PerformanceBenchmark()
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫
        report = benchmark.run_comprehensive_benchmark()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        save_benchmark_report(report)
        
        # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        print_benchmark_summary(report)
        
        logger.info("‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
        raise
    finally:
        benchmark.cleanup()


if __name__ == "__main__":
    main()