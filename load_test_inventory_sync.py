#!/usr/bin/env python3
"""
–ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤.

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç:
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
- –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ CPU
- –ü—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã
- –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π

–ê–≤—Ç–æ—Ä: ETL System
–î–∞—Ç–∞: 06 —è–Ω–≤–∞—Ä—è 2025
"""

import os
import sys
import time
import psutil
import threading
import asyncio
import json
import tempfile
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import gc
import tracemalloc
from unittest.mock import Mock, patch
import random
import string

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

try:
    from inventory_sync_service_optimized import OptimizedInventorySyncService, InventoryRecord, SyncResult, SyncStatus
    from parallel_sync_manager import ParallelSyncManager, SyncPriority
    from api_request_optimizer import APIRequestOptimizer, CacheType
    import config
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class LoadTestConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    # –†–∞–∑–º–µ—Ä—ã —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    small_dataset_size: int = 1000
    medium_dataset_size: int = 10000
    large_dataset_size: int = 100000
    xlarge_dataset_size: int = 500000
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    max_workers: int = 8
    batch_sizes: List[int] = None
    test_duration_minutes: int = 30
    
    # –ü–æ—Ä–æ–≥–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    max_memory_mb: int = 2048
    max_cpu_percent: float = 80.0
    min_throughput_per_second: float = 100.0
    max_sync_time_seconds: int = 300
    
    def __post_init__(self):
        if self.batch_sizes is None:
            self.batch_sizes = [100, 500, 1000, 2000, 5000]


@dataclass
class PerformanceMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
    test_name: str
    dataset_size: int
    batch_size: int
    workers_count: int
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: float = 0.0
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    records_processed: int = 0
    records_per_second: float = 0.0
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
    peak_memory_mb: float = 0.0
    avg_cpu_percent: float = 0.0
    peak_cpu_percent: float = 0.0
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    success_rate: float = 0.0
    error_count: int = 0
    cache_hit_rate: float = 0.0
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    api_requests_count: int = 0
    database_operations: int = 0
    
    def calculate_derived_metrics(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
        if self.end_time:
            self.duration_seconds = (self.end_time - self.start_time).total_seconds()
            if self.duration_seconds > 0:
                self.records_per_second = self.records_processed / self.duration_seconds


class ResourceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä —Ä–µ—Å—É—Ä—Å–æ–≤ —Å–∏—Å—Ç–µ–º—ã."""
    
    def __init__(self, interval_seconds: float = 1.0):
        self.interval_seconds = interval_seconds
        self.monitoring = False
        self.thread: Optional[threading.Thread] = None
        self.metrics: List[Dict[str, Any]] = []
        self._lock = threading.Lock()
    
    def start(self):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞."""
        if not self.monitoring:
            self.monitoring = True
            self.metrics.clear()
            self.thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self.thread.start()
            logger.info("üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –∑–∞–ø—É—â–µ–Ω")
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞."""
        self.monitoring = False
        if self.thread:
            self.thread.join(timeout=5)
        logger.info("üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def _monitor_loop(self):
        """–¶–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        process = psutil.Process()
        
        while self.monitoring:
            try:
                # –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
                cpu_percent = psutil.cpu_percent(interval=None)
                memory = psutil.virtual_memory()
                
                # –†–µ—Å—É—Ä—Å—ã –ø—Ä–æ—Ü–µ—Å—Å–∞
                process_memory = process.memory_info()
                process_cpu = process.cpu_percent()
                
                # –°–µ—Ç–µ–≤—ã–µ –∏ –¥–∏—Å–∫–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
                net_io = psutil.net_io_counters()
                disk_io = psutil.disk_io_counters()
                
                metric = {
                    'timestamp': datetime.now(),
                    'system_cpu_percent': cpu_percent,
                    'system_memory_percent': memory.percent,
                    'system_memory_mb': memory.used / 1024 / 1024,
                    'process_memory_mb': process_memory.rss / 1024 / 1024,
                    'process_cpu_percent': process_cpu,
                    'network_bytes_sent': net_io.bytes_sent if net_io else 0,
                    'network_bytes_recv': net_io.bytes_recv if net_io else 0,
                    'disk_read_bytes': disk_io.read_bytes if disk_io else 0,
                    'disk_write_bytes': disk_io.write_bytes if disk_io else 0
                }
                
                with self._lock:
                    self.metrics.append(metric)
                
                time.sleep(self.interval_seconds)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤: {e}")
                time.sleep(self.interval_seconds)
    
    def get_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º."""
        with self._lock:
            if not self.metrics:
                return {}
            
            cpu_values = [m['system_cpu_percent'] for m in self.metrics]
            memory_values = [m['process_memory_mb'] for m in self.metrics]
            
            return {
                'duration_seconds': (self.metrics[-1]['timestamp'] - self.metrics[0]['timestamp']).total_seconds(),
                'samples_count': len(self.metrics),
                'avg_cpu_percent': sum(cpu_values) / len(cpu_values),
                'peak_cpu_percent': max(cpu_values),
                'avg_memory_mb': sum(memory_values) / len(memory_values),
                'peak_memory_mb': max(memory_values),
                'memory_growth_mb': memory_values[-1] - memory_values[0] if len(memory_values) > 1 else 0
            }


class MockDataGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    
    @staticmethod
    def generate_ozon_inventory_data(size: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö Ozon."""
        logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {size} –∑–∞–ø–∏—Å–µ–π Ozon...")
        
        data = []
        warehouses = ['Ozon Main', 'Ozon FBS', 'Ozon Express']
        stock_types = ['FBO', 'FBS', 'realFBS']
        
        for i in range(size):
            offer_id = f"OZON_TEST_{i:06d}"
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –æ—Å—Ç–∞—Ç–∫–∏ –ø–æ —Å–∫–ª–∞–¥–∞–º
            stocks = []
            num_warehouses = random.randint(1, 3)
            
            for j in range(num_warehouses):
                warehouse = random.choice(warehouses)
                stock_type = random.choice(stock_types)
                present = random.randint(0, 1000)
                reserved = random.randint(0, min(present, 100))
                
                stocks.append({
                    'warehouse_name': warehouse,
                    'type': stock_type,
                    'present': present,
                    'reserved': reserved
                })
            
            data.append({
                'offer_id': offer_id,
                'stocks': stocks
            })
        
        logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π Ozon")
        return data
    
    @staticmethod
    def generate_wb_inventory_data(size: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö Wildberries."""
        logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {size} –∑–∞–ø–∏—Å–µ–π Wildberries...")
        
        data = []
        warehouses = ['WB Main', 'WB Express', 'WB Regional']
        
        for i in range(size):
            barcode = f"{''.join(random.choices(string.digits, k=13))}"
            nm_id = 1000000 + i
            
            quantity = random.randint(0, 1000)
            in_way_to_client = random.randint(0, min(quantity, 50))
            
            data.append({
                'barcode': barcode,
                'nmId': nm_id,
                'warehouseName': random.choice(warehouses),
                'quantity': quantity,
                'inWayToClient': in_way_to_client
            })
        
        logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π Wildberries")
        return data


class LoadTester:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    
    def __init__(self, config: LoadTestConfig):
        self.config = config
        self.results: List[PerformanceMetrics] = []
        self.temp_dir = tempfile.mkdtemp()
        
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    def test_batch_processing_performance(self) -> List[PerformanceMetrics]:
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        results = []
        dataset_sizes = [
            self.config.small_dataset_size,
            self.config.medium_dataset_size,
            self.config.large_dataset_size
        ]
        
        for dataset_size in dataset_sizes:
            for batch_size in self.config.batch_sizes:
                logger.info(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º: —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö={dataset_size}, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞={batch_size}")
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                test_data = MockDataGenerator.generate_ozon_inventory_data(dataset_size)
                
                # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —Å –º–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ –ë–î
                service = OptimizedInventorySyncService(
                    batch_size=batch_size,
                    max_workers=self.config.max_workers
                )
                
                # –ú–æ–∫–∏—Ä—É–µ–º –∫—ç—à —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                service.product_cache.get_product_id_by_ozon_sku = Mock(return_value=1)
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
                monitor = ResourceMonitor(interval_seconds=0.5)
                monitor.start()
                
                # –í–∫–ª—é—á–∞–µ–º —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É –ø–∞–º—è—Ç–∏
                tracemalloc.start()
                
                try:
                    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç
                    start_time = datetime.now()
                    
                    processed_records = service.process_inventory_batch(test_data, 'Ozon')
                    
                    end_time = datetime.now()
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏
                    current, peak = tracemalloc.get_traced_memory()
                    tracemalloc.stop()
                    
                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                    monitor.stop()
                    resource_summary = monitor.get_summary()
                    
                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    metrics = PerformanceMetrics(
                        test_name="batch_processing",
                        dataset_size=dataset_size,
                        batch_size=batch_size,
                        workers_count=self.config.max_workers,
                        start_time=start_time,
                        end_time=end_time,
                        records_processed=len(processed_records),
                        peak_memory_mb=peak / 1024 / 1024,
                        avg_cpu_percent=resource_summary.get('avg_cpu_percent', 0),
                        peak_cpu_percent=resource_summary.get('peak_cpu_percent', 0),
                        success_rate=1.0 if processed_records else 0.0
                    )
                    
                    metrics.calculate_derived_metrics()
                    results.append(metrics)
                    
                    logger.info(f"‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {metrics.records_per_second:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫, "
                               f"–ø–∏–∫ –ø–∞–º—è—Ç–∏: {metrics.peak_memory_mb:.1f} –ú–ë")
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")
                    monitor.stop()
                    tracemalloc.stop()
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
                gc.collect()
                time.sleep(2)
        
        return results
    
    def test_concurrent_sync_performance(self) -> List[PerformanceMetrics]:
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
        
        results = []
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤
        worker_counts = [1, 2, 4, 8]
        dataset_size = self.config.medium_dataset_size
        
        for workers in worker_counts:
            logger.info(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é —Å {workers} –≤–æ—Ä–∫–µ—Ä–∞–º–∏")
            
            # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
            manager = ParallelSyncManager(
                max_workers=max(1, workers),  # –ú–∏–Ω–∏–º—É–º 1 –≤–æ—Ä–∫–µ—Ä
                max_concurrent_marketplaces=2,
                resource_monitoring=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            )
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
            monitor = ResourceMonitor(interval_seconds=0.5)
            monitor.start()
            
            tracemalloc.start()
            
            try:
                start_time = datetime.now()
                
                # –ú–æ–∫–∏—Ä—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                with patch.object(OptimizedInventorySyncService, 'sync_ozon_inventory_optimized') as mock_sync:
                    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–∫ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ —É—Å–ø–µ—à–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    mock_result = SyncResult(
                        source='Ozon',
                        status=SyncStatus.SUCCESS,
                        records_processed=dataset_size,
                        records_updated=0,
                        records_inserted=dataset_size,
                        records_failed=0,
                        started_at=start_time,
                        completed_at=None
                    )
                    mock_sync.return_value = mock_result
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    sync_results = loop.run_until_complete(
                        manager.run_parallel_sync(['Ozon'], wait_for_completion=True)
                    )
                    
                    loop.close()
                
                end_time = datetime.now()
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                current, peak = tracemalloc.get_traced_memory()
                tracemalloc.stop()
                
                monitor.stop()
                resource_summary = monitor.get_summary()
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics = PerformanceMetrics(
                    test_name="concurrent_sync",
                    dataset_size=dataset_size,
                    batch_size=1000,  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
                    workers_count=workers,
                    start_time=start_time,
                    end_time=end_time,
                    records_processed=dataset_size,
                    peak_memory_mb=peak / 1024 / 1024,
                    avg_cpu_percent=resource_summary.get('avg_cpu_percent', 0),
                    peak_cpu_percent=resource_summary.get('peak_cpu_percent', 0),
                    success_rate=1.0 if sync_results else 0.0
                )
                
                metrics.calculate_derived_metrics()
                results.append(metrics)
                
                logger.info(f"‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {metrics.records_per_second:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞: {e}")
                monitor.stop()
                tracemalloc.stop()
            finally:
                manager.cleanup()
            
            gc.collect()
            time.sleep(2)
        
        return results
    
    def test_memory_usage_scaling(self) -> List[PerformanceMetrics]:
        """–¢–µ—Å—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏")
        
        results = []
        dataset_sizes = [
            self.config.small_dataset_size,
            self.config.medium_dataset_size,
            self.config.large_dataset_size,
            self.config.xlarge_dataset_size
        ]
        
        for dataset_size in dataset_sizes:
            logger.info(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è {dataset_size} –∑–∞–ø–∏—Å–µ–π")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_data = MockDataGenerator.generate_ozon_inventory_data(dataset_size)
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å
            service = OptimizedInventorySyncService(
                batch_size=1000,
                max_workers=4
            )
            service.product_cache.get_product_id_by_ozon_sku = Mock(return_value=1)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
            monitor = ResourceMonitor(interval_seconds=0.1)
            monitor.start()
            
            tracemalloc.start()
            
            try:
                start_time = datetime.now()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                processed_records = service.process_inventory_batch(test_data, 'Ozon')
                
                end_time = datetime.now()
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–∏–∫–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                current, peak = tracemalloc.get_traced_memory()
                tracemalloc.stop()
                
                monitor.stop()
                resource_summary = monitor.get_summary()
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics = PerformanceMetrics(
                    test_name="memory_scaling",
                    dataset_size=dataset_size,
                    batch_size=1000,
                    workers_count=4,
                    start_time=start_time,
                    end_time=end_time,
                    records_processed=len(processed_records),
                    peak_memory_mb=peak / 1024 / 1024,
                    avg_cpu_percent=resource_summary.get('avg_cpu_percent', 0),
                    peak_cpu_percent=resource_summary.get('peak_cpu_percent', 0),
                    success_rate=1.0 if processed_records else 0.0
                )
                
                metrics.calculate_derived_metrics()
                results.append(metrics)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —É—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏
                memory_per_record = metrics.peak_memory_mb / dataset_size * 1024  # –ö–ë –Ω–∞ –∑–∞–ø–∏—Å—å
                
                logger.info(f"‚úÖ –ü–∞–º—è—Ç—å –¥–ª—è {dataset_size} –∑–∞–ø–∏—Å–µ–π: {metrics.peak_memory_mb:.1f} –ú–ë "
                           f"({memory_per_record:.2f} –ö–ë/–∑–∞–ø–∏—Å—å)")
                
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –≤—ã—Å–æ–∫–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏
                if metrics.peak_memory_mb > self.config.max_memory_mb:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: {metrics.peak_memory_mb:.1f} –ú–ë > {self.config.max_memory_mb} –ú–ë")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞ –ø–∞–º—è—Ç–∏: {e}")
                monitor.stop()
                tracemalloc.stop()
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
            del test_data
            gc.collect()
            time.sleep(3)
        
        return results
    
    def test_database_performance(self) -> List[PerformanceMetrics]:
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–ø–µ—Ä–∞—Ü–∏–π —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ë–î")
        
        results = []
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–µ–π –¥–ª—è –ë–î –æ–ø–µ—Ä–∞—Ü–∏–π
        batch_sizes = [100, 500, 1000, 2000, 5000]
        dataset_size = self.config.medium_dataset_size
        
        for batch_size in batch_sizes:
            logger.info(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –ë–î –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –±–∞—Ç—á–µ–º {batch_size}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
            test_records = []
            for i in range(dataset_size):
                record = InventoryRecord(
                    product_id=i + 1,
                    sku=f'TEST_{i:06d}',
                    source='Ozon',
                    warehouse_name='Test Warehouse',
                    stock_type='FBO',
                    current_stock=random.randint(0, 1000),
                    reserved_stock=random.randint(0, 100),
                    available_stock=random.randint(0, 900),
                    quantity_present=random.randint(0, 1000),
                    quantity_reserved=random.randint(0, 100),
                    snapshot_date=datetime.now().date()
                )
                test_records.append(record)
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —Å –º–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ë–î
            service = OptimizedInventorySyncService(batch_size=batch_size)
            
            # –ú–æ–∫–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
            mock_connection = Mock()
            mock_cursor = Mock()
            service.connection = mock_connection
            service.cursor = mock_cursor
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–∫ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –ë–î –æ–ø–µ—Ä–∞—Ü–∏–π
            mock_cursor.rowcount = 0
            mock_cursor.executemany = Mock()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            monitor = ResourceMonitor(interval_seconds=0.5)
            monitor.start()
            
            try:
                start_time = datetime.now()
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
                updated, inserted, failed = service.batch_upsert_inventory_data(test_records, 'Ozon')
                
                end_time = datetime.now()
                
                monitor.stop()
                resource_summary = monitor.get_summary()
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ë–î –æ–ø–µ—Ä–∞—Ü–∏–π
                db_operations = mock_cursor.executemany.call_count
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics = PerformanceMetrics(
                    test_name="database_performance",
                    dataset_size=dataset_size,
                    batch_size=batch_size,
                    workers_count=1,
                    start_time=start_time,
                    end_time=end_time,
                    records_processed=dataset_size,
                    avg_cpu_percent=resource_summary.get('avg_cpu_percent', 0),
                    peak_cpu_percent=resource_summary.get('peak_cpu_percent', 0),
                    success_rate=1.0,
                    database_operations=db_operations
                )
                
                metrics.calculate_derived_metrics()
                results.append(metrics)
                
                logger.info(f"‚úÖ –ë–î —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {metrics.records_per_second:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫, "
                           f"{db_operations} –ë–î –æ–ø–µ—Ä–∞—Ü–∏–π")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ë–î —Ç–µ—Å—Ç–∞: {e}")
                monitor.stop()
            
            gc.collect()
        
        return results
    
    def run_full_load_test_suite(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤."""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤")
        
        start_time = datetime.now()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
        batch_results = self.test_batch_processing_performance()
        concurrent_results = self.test_concurrent_sync_performance()
        memory_results = self.test_memory_usage_scaling()
        db_results = self.test_database_performance()
        
        end_time = datetime.now()
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_results = batch_results + concurrent_results + memory_results + db_results
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analysis = self._analyze_results(all_results)
        
        # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report = {
            'test_summary': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_duration_minutes': (end_time - start_time).total_seconds() / 60,
                'total_tests': len(all_results),
                'config': asdict(self.config)
            },
            'test_results': {
                'batch_processing': [asdict(r) for r in batch_results],
                'concurrent_sync': [asdict(r) for r in concurrent_results],
                'memory_scaling': [asdict(r) for r in memory_results],
                'database_performance': [asdict(r) for r in db_results]
            },
            'performance_analysis': analysis,
            'recommendations': self._generate_recommendations(analysis)
        }
        
        logger.info("‚úÖ –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω")
        return report
    
    def _analyze_results(self, results: List[PerformanceMetrics]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        if not results:
            return {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–∏–ø–∞–º —Ç–µ—Å—Ç–æ–≤
        by_test_type = {}
        for result in results:
            test_type = result.test_name
            if test_type not in by_test_type:
                by_test_type[test_type] = []
            by_test_type[test_type].append(result)
        
        analysis = {}
        
        for test_type, test_results in by_test_type.items():
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            throughputs = [r.records_per_second for r in test_results]
            memory_usage = [r.peak_memory_mb for r in test_results]
            cpu_usage = [r.avg_cpu_percent for r in test_results]
            
            analysis[test_type] = {
                'count': len(test_results),
                'throughput': {
                    'min': min(throughputs),
                    'max': max(throughputs),
                    'avg': sum(throughputs) / len(throughputs),
                    'median': sorted(throughputs)[len(throughputs) // 2]
                },
                'memory_usage': {
                    'min_mb': min(memory_usage),
                    'max_mb': max(memory_usage),
                    'avg_mb': sum(memory_usage) / len(memory_usage)
                },
                'cpu_usage': {
                    'min_percent': min(cpu_usage),
                    'max_percent': max(cpu_usage),
                    'avg_percent': sum(cpu_usage) / len(cpu_usage)
                },
                'performance_issues': []
            }
            
            # –í—ã—è–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            issues = analysis[test_type]['performance_issues']
            
            if max(throughputs) < self.config.min_throughput_per_second:
                issues.append(f"–ù–∏–∑–∫–∞—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {max(throughputs):.1f} < {self.config.min_throughput_per_second}")
            
            if max(memory_usage) > self.config.max_memory_mb:
                issues.append(f"–ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ –ø–∞–º—è—Ç–∏: {max(memory_usage):.1f} –ú–ë > {self.config.max_memory_mb} –ú–ë")
            
            if max(cpu_usage) > self.config.max_cpu_percent:
                issues.append(f"–í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU: {max(cpu_usage):.1f}% > {self.config.max_cpu_percent}%")
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        recommendations = []
        
        for test_type, stats in analysis.items():
            if stats['performance_issues']:
                recommendations.append(f"–ü—Ä–æ–±–ª–µ–º—ã –≤ —Ç–µ—Å—Ç–µ {test_type}:")
                for issue in stats['performance_issues']:
                    recommendations.append(f"  - {issue}")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if any('memory' in issue.lower() for test_stats in analysis.values() for issue in test_stats['performance_issues']):
            recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–∞–º—è—Ç–∏:")
            recommendations.append("  - –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–µ–π –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤")
            recommendations.append("  - –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
            recommendations.append("  - –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—É—é —Å–±–æ—Ä–∫—É –º—É—Å–æ—Ä–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏")
        
        if any('cpu' in issue.lower() for test_stats in analysis.values() for issue in test_stats['performance_issues']):
            recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ CPU:")
            recommendations.append("  - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            recommendations.append("  - –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤")
            recommendations.append("  - –î–æ–±–∞–≤–∏—Ç—å –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏")
        
        if any('throughput' in issue.lower() for test_stats in analysis.values() for issue in test_stats['performance_issues']):
            recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
            recommendations.append("  - –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–µ–π")
            recommendations.append("  - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            recommendations.append("  - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —á–∞—Å—Ç–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        return recommendations


def save_test_report(report: Dict[str, Any], filename: str = None):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏."""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"load_test_report_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")


def print_test_summary(report: Dict[str, Any]):
    """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    print("\n" + "="*80)
    print("–°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ù–ê–ì–†–£–ó–û–ß–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*80)
    
    summary = report['test_summary']
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {summary['total_duration_minutes']:.1f} –º–∏–Ω—É—Ç")
    print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {summary['total_tests']}")
    
    print("\n–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –¢–ò–ü–ê–ú –¢–ï–°–¢–û–í:")
    print("-"*40)
    
    analysis = report['performance_analysis']
    for test_type, stats in analysis.items():
        print(f"\n{test_type.upper()}:")
        print(f"  –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {stats['throughput']['avg']:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
        print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {stats['memory_usage']['avg_mb']:.1f} –ú–ë")
        print(f"  –ó–∞–≥—Ä—É–∑–∫–∞ CPU: {stats['cpu_usage']['avg_percent']:.1f}%")
        
        if stats['performance_issues']:
            print(f"  ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã: {len(stats['performance_issues'])}")
    
    print("\n–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print("-"*40)
    recommendations = report['recommendations']
    if recommendations:
        for rec in recommendations:
            print(f"  {rec}")
    else:
        print("  ‚úÖ –ü—Ä–æ–±–ª–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ")
    
    print("\n" + "="*80)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤."""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    config = LoadTestConfig(
        small_dataset_size=1000,
        medium_dataset_size=10000,
        large_dataset_size=50000,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        xlarge_dataset_size=100000,
        max_workers=8,
        batch_sizes=[100, 500, 1000, 2000],
        max_memory_mb=1024,  # 1 –ì–ë –ª–∏–º–∏—Ç
        max_cpu_percent=85.0,
        min_throughput_per_second=50.0
    )
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–µ—Ä
    tester = LoadTester(config)
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤
        report = tester.run_full_load_test_suite()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        save_test_report(report)
        
        # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        print_test_summary(report)
        
        logger.info("‚úÖ –ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        raise
    finally:
        tester.cleanup()


if __name__ == "__main__":
    main()